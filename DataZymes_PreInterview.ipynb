{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING LIBRARIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np \n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Fake Header in  order to parse the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "'User-Agent': 'Mozilla/4.0 (Macintosh; Intel Mac OS X 10.13; rv:60.0) Gecko/20100101 Firefox/60.0'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few functions for PageVistits , FindingUrl, Finding specific class,finding Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for visiting a URL and returning the full page\n",
    "def pagevists(url):\n",
    "    html_content = requests.get(url, headers=headers)\n",
    "    html_content.status_code\n",
    "    data=html_content.text\n",
    "    soup=BeautifulSoup(data,\"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A program in which we can pass arguments as the full page and what to find and the tags\n",
    "def find(data,find,form,**kwargs):\n",
    "    return data.findAll(find,form)\n",
    "#This function finds all the links present in a page or in a piece of html tags , basically I have \n",
    "#build this to find inside a block of html tags passed as list \n",
    "def linkfinder(val):\n",
    "    url=[]\n",
    "    for link in range (len(val)):\n",
    "        l=val[link]\n",
    "        k=l.find('a')\n",
    "        url.append(k.get('href'))\n",
    "    return(url)\n",
    "#This function builds the broken url so that we can visit the page \n",
    "def urlbuilder(val):\n",
    "    \n",
    "    for i in range(len(val)):\n",
    "        val[i]='https://health.usnews.com'+val[i]\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting the page \n",
    "url = 'https://health.usnews.com/doctors/city-index/new-jersey'\n",
    "soup=pagevists(url)\n",
    "#Here I am using Multiple Fuction in one go i.e \n",
    "#first getting the links block\n",
    "#then find the the links \n",
    "#finally rebuilding the links \n",
    "#here set and list is used to avoid any repetition\n",
    "\n",
    "#All the links for city \n",
    "citylinks=list (set (urlbuilder(linkfinder(find(soup,'li',{'class':'List__ListItem-dl3d8e-1 kfaWAY'})))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we have found all the links now we will visit each link and also simultanouesly start building the elasticsearch documents of each doctor first we will create some more functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following piece of code was to add  Load More functionality somehow I was getting blocked everytime and as beautiful soup doesnot support page clicks and hence it was not possibe,\n",
    "I tried to open via proxy server , it worked fine but still integrating it to python was difficult ,Using \n",
    "of selenium in this case should have solved the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def doc_finder(data,link):\n",
    "#     l=[]\n",
    "#     import sys\n",
    "#     m=1\n",
    "    \n",
    "#     while(m):\n",
    "            \n",
    "#             m=data.find('a',{'rel':'next' })\n",
    "        \n",
    "#                 url='http://health.usnews.com'+m.get('href')\n",
    "#             except:\n",
    "#                 url=link\n",
    "            \n",
    "#             l.append(url)\n",
    "#             data=pagevists(url)\n",
    "#     if len(l)==0:\n",
    "#         data=pagevists(l[0])\n",
    "#         return data\n",
    "#     else:\n",
    "#         data=pagevists(l[len(l)-2])\n",
    "#         return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used in the doctor's list page where this will find the links of all doctor in that page \n",
    "def doc_page(val):\n",
    "    doc=[]\n",
    "    for link in range (len(val)):\n",
    "        doc.append(val[link].get('href'))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are all the function which are scrapping data from the doctor's page and I have used regex to remove blank\n",
    "#spaces and also to remove \\n\n",
    "#All these function is finding the class of the tag and returning the text format\n",
    "\n",
    "def name_scrap(data):\n",
    "    name=data.find_all('h1',{'class':'hero-heading flex-media-heading block-tight doctor-name '})\n",
    "    for i in name :\n",
    "        return re.sub(\"\\s\\s+\" , \" \", i.text)\n",
    "def speciality(data):\n",
    "    name=data.find_all('a',{'class':'bar-flush '})\n",
    "    for i in name :\n",
    "        return re.sub(\"\\s\\s+\" , \" \", i.text)\n",
    "def city(data):\n",
    "    name=data.find_all('strong')\n",
    "    for i in name :\n",
    "        return re.sub(\"\\s\\s+\" , \" \", i.text)\n",
    "def overview(data):\n",
    "    name=data.find_all('div',{'class':'block-normal clearfix'})\n",
    "    for i in name :\n",
    "        return re.sub(\"\\s\\s+\" , \" \", i.text)\n",
    "def experiance(data):\n",
    "    name=data.find_all('span',{'class':'text-large heading-normal-for-small-only right-for-medium-up'})\n",
    "    lm=[]\n",
    "    for i in name:\n",
    "        lm.append(i.text)\n",
    "    return (lm[1])\n",
    "def languages(data):\n",
    "    name=data.find_all('span',{'class':'text-large heading-normal-for-small-only right-for-medium-up text-right showmore'})\n",
    "    for i in name :\n",
    "        return re.sub(\"\\s\\s+\" , \" \", i.text)\n",
    "def Address(data):\n",
    "    name=data.find_all('span',{'data-js-id':'doctor-address'})\n",
    "    for i in name :\n",
    "        return re.sub(\"\\s\\s+\" , \" \", i.text)\n",
    "def affiliation(data):\n",
    "    name=data.find_all('a',{'data-tracking-placement':'hospital-affiliation-in-content'})\n",
    "    for i in name :\n",
    "        return re.sub(\"\\s\\s+\" , \" \", i.text)\n",
    "def PinFinder(data):\n",
    "    l=Address(data)\n",
    "    lm=l.split()\n",
    "    return lm[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Elastic Search and connecting it with the local host \n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host': 'localhost','port': 9200}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  This is the code block which is doing the whole scrapping and storing each document as a elasticsearch document : below is the detailed explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step1:  The first for loop is visiting the city in a iterative way, and as the url contains which city we are \n",
    "visiting hence I am spliting it and storing as list\n",
    "\n",
    "\n",
    "step2: Now I have visited the City page , and also scrapped all the specialised links from there \n",
    "\n",
    "step3: second loop is traversing each specialisation link one by one and getting the links of all doctors from there\n",
    "\n",
    "step4:As I have the doctors links I visited the links iterativly and scrapped the necessary data ,\n",
    "\n",
    "step5:Finally I  created a json document and pushed that into elasticsearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "n=1\n",
    "from IPython.display import clear_output\n",
    "#The range should be the length of citylinks ,here only 5 is being taken for time limitation\n",
    "for i in range(5):\n",
    "    soup=pagevists(citylinks[i])\n",
    "    soup=pagevists(citylinks[i])\n",
    "    c=citylinks[i].split('/')\n",
    "#Used for reducing load time on the server \n",
    "    time.sleep(np.random.choice([1,4,2]))\n",
    "\n",
    "#Finding all the specialisation links of a city    \n",
    "    next_page=list (set (urlbuilder(linkfinder(find(soup,'li',{'class':'List__ListItem-dl3d8e-1 kfaWAY'})))))\n",
    "    sp=next_page[i].split('/')\n",
    "#The range should be the length of next_page ,only 1 is taken again for time limitation\n",
    "    for j in range(1):\n",
    "        data=pagevists(next_page[j])\n",
    "#Used for reducing load time on the server \n",
    "        time.sleep(np.random.choice([1,4,2]))\n",
    "        doc_index=[]\n",
    "#Getting all the links of doctor's\n",
    "        doc_index.append(urlbuilder(doc_page(find(data,'a',{'class':'search-result-link bar-tighter'}))))\n",
    "#For converting list of list to only list\n",
    "        doc_index=doc_index[0]\n",
    "        for k in range (len(doc_index)):\n",
    "            \n",
    "            data=pagevists(doc_index[k])\n",
    "            time.sleep(np.random.choice([1,4,2]))\n",
    "#Creating Json Document \n",
    "            document={\n",
    "                'Doctor_Name':name_scrap(data),\n",
    "                'Speciality':speciality(data),\n",
    "                'City':c[-1],\n",
    "                'overview':overview(data),\n",
    "                'Experiance':experiance(data),\n",
    "                'Languages':languages(data),\n",
    "                'Location':Address(data),\n",
    "                'Affiliation':affiliation(data),\n",
    "                'Pin':PinFinder(data)\n",
    "            }\n",
    "#Finally Pushing it \n",
    "            es.index(index='doc_profile',doc_type='docs',id=n,body=document)\n",
    "#Just for checking the counts and if the above code works properly\n",
    "            clear_output()\n",
    "            n=n+1\n",
    "            print (n)\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification if entered properly or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': u'1',\n",
       " u'_index': u'doc_profile',\n",
       " u'_source': {u'Affiliation': None,\n",
       "  u'City': u'jamesburg',\n",
       "  u'Doctor_Name': u' Dr. Stuart Rubenstein MD\\n',\n",
       "  u'Experiance': u'21+',\n",
       "  u'Languages': u'\\nEnglish\\n',\n",
       "  u'Location': u' 29 Harwood Road Monroe Township, NJ 08831 ',\n",
       "  u'Pin': u'08831',\n",
       "  u'Speciality': u'Internal Medicine',\n",
       "  u'overview': u' Dr. Stuart Rubenstein is an internist in Monroe Township, New Jersey. He received his medical degree from State University of New York Downstate Medical Center College of Medicine and has been in practice for more than 20 years. '},\n",
       " u'_type': u'docs',\n",
       " u'_version': 1,\n",
       " u'found': True}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.get(index='doc_profile',doc_type='docs',id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval using Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl.connections import connections\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "\n",
    "# Define a default Elasticsearch client\n",
    "client = connections.create_connection(hosts=['http://localhost:9200'])\n",
    "#Defining the elasticsearch Search method\n",
    "s = Search(using=client, index='doc_profile')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Of the Data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Finding Doctors per city "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elizabeth 21\n",
      "fair-lawn 20\n",
      "jamesburg 20\n",
      "hightstown 12\n",
      "kinnelon 9\n"
     ]
    }
   ],
   "source": [
    "s.aggs.bucket('Doc_City', 'terms', field='City.keyword', size=1000)\n",
    "t=s.execute()\n",
    "for i in t.aggregations.Doc_City.buckets:\n",
    "    print i.key , i.doc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Finding Number of Doctors by speciality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Search(using=client, index='doc_profile')\n",
    "s.aggs.bucket('Speciality', 'terms', field='Specialitry.keyword', size=1000)\n",
    "t=s.execute()\n",
    "for i in t.aggregations.Speciality.buckets:\n",
    "    print i.key , i.doc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Finding Number of doctors by Experiance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21+ 54\n",
      "11 - 20 17\n",
      "6 - 10 7\n",
      "1 - 2 2\n",
      "3 - 5 1\n",
      "Male 1\n"
     ]
    }
   ],
   "source": [
    "s = Search(using=client, index='doc_profile')\n",
    "s.aggs.bucket('Exp', 'terms', field='Experiance.keyword', size=1000)\n",
    "t=s.execute()\n",
    "for i in t.aggregations.Exp.buckets:\n",
    "    print i.key , i.doc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Finding the number of doctors by PIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08831 18\n",
      "10032 10\n",
      "07601 7\n",
      "07202 5\n",
      "07208 4\n",
      "07103 3\n",
      "07111 3\n",
      "07112 3\n",
      "08628 3\n",
      "07503 2\n",
      "08625 2\n",
      "07016 1\n",
      "07039 1\n",
      "07054 1\n",
      "07083 1\n",
      "07204 1\n",
      "07207 1\n",
      "07666 1\n",
      "07677 1\n",
      "07722 1\n",
      "07746 1\n",
      "07856 1\n",
      "08540 1\n",
      "08542 1\n",
      "08558 1\n",
      "08618 1\n",
      "08857 1\n",
      "08884 1\n",
      "10022 1\n",
      "10039 1\n",
      "10467 1\n",
      "19067 1\n",
      "19127 1\n"
     ]
    }
   ],
   "source": [
    "s = Search(using=client, index='doc_profile')\n",
    "s.aggs.bucket('PIN', 'terms', field='Pin.keyword', size=1000)\n",
    "t=s.execute()\n",
    "for i in t.aggregations.PIN.buckets:\n",
    "    print i.key , i.doc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note : I can only retrieve data from the 1st page of doctors's List ie. https://health.usnews.com/doctors/anesthesiologists/new-jersey/absecon , As the Load More link was not opening by any python native packages I researched and found It can be  opened by a proxy server , but I do not have access to a proxy server , third party libraries could have solve the problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='Center'> The end <h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
